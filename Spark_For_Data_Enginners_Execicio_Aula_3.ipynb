{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1gu0rVsMk3t9M1ZfJLnw69XYXKBqsikd2",
      "authorship_tag": "ABX9TyNUhZxPU99LkDx4DN55ZLI+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guisilcol/spark-for-data-enginners/blob/main/Spark_For_Data_Enginners_Execicio_Aula_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalação de módulos \n",
        "\n"
      ],
      "metadata": {
        "id": "jNadjCWLVu_o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYqT2XObU7ZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d88b62d4-053f-4631-f10b-3d46d236725d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 54 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 68.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=f429bd50e76350455e536cdab7afdb87b84eef9cfb8629f5d5e2adc2907b6b3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bibtexparser\n",
            "  Downloading bibtexparser-1.4.0.tar.gz (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 680 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from bibtexparser) (3.0.9)\n",
            "Building wheels for collected packages: bibtexparser\n",
            "  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bibtexparser: filename=bibtexparser-1.4.0-py3-none-any.whl size=42443 sha256=bc0aa2622195733de0ec8291e2c89ba40b9e1533755f9ff50d35e689ff263907\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/b1/30/eea6f0a0ecf1af0d19348e0f6928462817e9176e6b71059dfb\n",
            "Successfully built bibtexparser\n",
            "Installing collected packages: bibtexparser\n",
            "Successfully installed bibtexparser-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install bibtexparser\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importação de módulos e inicialização de constantes"
      ],
      "metadata": {
        "id": "G5gzxDWhZLrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import pyspark.sql as spark_sql\n",
        "import pyspark.sql.functions as F\n",
        "import typing as python_types\n",
        "import pyspark.sql.types as spark_types\n",
        "\n",
        "ACM_BIBTEXS_INPUT_FOLDER = '/content/drive/MyDrive/Impacta/Spark for Data Enginners/Exercicio Aula 3/data/acm/*'\n",
        "IEEE_BIBTEXS_INPUT_FOLDER = '/content/drive/MyDrive/Impacta/Spark for Data Enginners/Exercicio Aula 3/data/ieee/*'\n",
        "SCIENCE_DIRECT_BIBTEXS_INPUT_FOLDER = '/content/drive/MyDrive/Impacta/Spark for Data Enginners/Exercicio Aula 3/data/science_direct/*'\n",
        "\n",
        "JCR_FILEPATH = '/content/drive/MyDrive/Impacta/Spark for Data Enginners/Exercicio Aula 3/data/jcs_2020.csv'\n",
        "SCIMAGO_FILEPATH = '/content/drive/MyDrive/Impacta/Spark for Data Enginners/Exercicio Aula 3/data/scimagojr 2020.csv'\n",
        "\n",
        "JSON_OUTPUT_FILE = '/content/drive/MyDrive/Impacta/Spark for Data Enginners/Exercicio Aula 3/output3'\n",
        "\n",
        "\n",
        "SPARK = spark_sql.SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "43F2p48RZYcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UDFs e Python Functions"
      ],
      "metadata": {
        "id": "4msPgogmbei4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob as get_filenames_in_folder\n",
        "from bibtexparser import load as load_bibtex_database\n",
        "\n",
        "class BibtexHandler:\n",
        "  \n",
        "  @staticmethod\n",
        "  def parse_bibtex_folder_in_dict_list(folder_path: str):\n",
        "    filepaths = get_filenames_in_folder(folder_path)\n",
        "    files = (open(path, \"r\", encoding=\"utf-8\") for path in filepaths)  # type: ignore\n",
        "    bibtexts = (load_bibtex_database(file) for file in files)\n",
        "    bib_entries = (bib.entries for bib in bibtexts)\n",
        "    return list((item for sublist in bib_entries for item in sublist))\n",
        "\n",
        "class Operations:\n",
        "\n",
        "  @staticmethod\n",
        "  def generate_key_from_journal_name(df: spark_sql.DataFrame, journal_title_column: str, output_column: str):\n",
        "    return df.withColumn(output_column, F.regexp_replace(journal_title_column, \"&\", \"AND\"))\\\n",
        "              .withColumn(output_column, F.regexp_replace(output_column, r\"([^A-Za-z0-9]+)\", \"\"))\\\n",
        "              .withColumn(output_column, F.upper(output_column))\\\n",
        "              .withColumn(output_column, F.trim((output_column)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hFRG8LN2bi8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extração de dados (Python side)"
      ],
      "metadata": {
        "id": "wYoKCq9Wav6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACM_INPUT_DATA: python_types.List[dict] = BibtexHandler.parse_bibtex_folder_in_dict_list(ACM_BIBTEXS_INPUT_FOLDER)\n",
        "IEEE_INPUT_DATA: python_types.List[dict] = BibtexHandler.parse_bibtex_folder_in_dict_list(IEEE_BIBTEXS_INPUT_FOLDER)\n",
        "SCIENCE_DIRECT_INPUT_DATA: python_types.List[dict] = BibtexHandler.parse_bibtex_folder_in_dict_list(SCIENCE_DIRECT_BIBTEXS_INPUT_FOLDER)"
      ],
      "metadata": {
        "id": "c3p-HO8QavZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extração e Transformação de dados\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pyv-46oXlrXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################## Criação dos Dataframes ##########################################\n",
        "ACM_DF = SPARK.createDataFrame(ACM_INPUT_DATA).cache() # type: ignore\n",
        "IEEE_DF = SPARK.createDataFrame(IEEE_INPUT_DATA).cache() # type: ignore\n",
        "SCIENCE_DIRECT_DF = SPARK.createDataFrame(SCIENCE_DIRECT_INPUT_DATA).cache() # type: ignore\n",
        "\n",
        "JCS_DF = SPARK.read.csv(JCR_FILEPATH, sep = \";\", header = True)\n",
        "SCIMAGO_DF = SPARK.read.csv(SCIMAGO_FILEPATH, sep = \";\", header = True)"
      ],
      "metadata": {
        "id": "dPhQYJb9Vkme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "########################################## Tratamento inicial dos bibtexs ##########################################\n",
        "acm_df = ACM_DF.select([\"author\", \"title\", \"keywords\", \"abstract\", \"year\", \"ENTRYTYPE\", \"doi\", \"issn\", \"isbn\", \"journal\"])\\\n",
        "        .withColumnRenamed(\"ENTRYTYPE\", \"type_publication\")\\\n",
        "        .withColumn(\"source\", F.lit(\"acm\"))\\\n",
        "        .withColumn(\"issn\", F.regexp_replace(\"issn\", \"-\", \"\"))\\\n",
        "        .dropDuplicates()\n",
        "\n",
        "ieee_df = IEEE_DF.select([\"author\", \"title\", \"keywords\", \"abstract\", \"year\", \"ENTRYTYPE\", \"doi\", \"issn\", \"journal\"])\\\n",
        "          .withColumnRenamed(\"ENTRYTYPE\", \"type_publication\")\\\n",
        "          .withColumn(\"source\", F.lit(\"ieee\"))\\\n",
        "          .withColumn(\"issn\", F.regexp_replace(\"issn\", \"-\", \"\"))\\\n",
        "          .withColumn(\"isbn\", F.lit(None).cast(spark_types.StringType()))\\\n",
        "          .dropDuplicates()\n",
        "\n",
        "science_direct_df = SCIENCE_DIRECT_DF.select([\"author\", \"title\", \"keywords\", \"abstract\", \"year\", \"ENTRYTYPE\", \"doi\", \"issn\", \"isbn\", \"journal\"])\\\n",
        "                    .withColumnRenamed(\"ENTRYTYPE\", \"type_publication\")\\\n",
        "                    .withColumn(\"source\", F.lit(\"science direct\"))\\\n",
        "                    .withColumn(\"issn\", F.regexp_replace(\"issn\", \"-\", \"\"))\\\n",
        "                    .dropDuplicates()\n",
        "\n",
        "########################################## Union dos bibtex ##########################################\n",
        "\n",
        "bibtex_df = acm_df.union(ieee_df).union(science_direct_df)"
      ],
      "metadata": {
        "id": "XToLlW6ylumw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################## Tratamento dos Bibtex ##########################################\n",
        "bibtex_df = bibtex_df.transform(Operations.generate_key_from_journal_name, \n",
        "                                  journal_title_column = 'journal', \n",
        "                                  output_column = 'journal_name_key')\\\n",
        "                      .withColumn('doi', F.regexp_replace('doi', 'https://doi.org/', ''))\n",
        "\n",
        "########################################## Transformação dos arquivos CSV ##########################################\n",
        "scimago_df = SCIMAGO_DF.select(['Issn', 'Title', 'SJR'])\\\n",
        "                        .withColumnRenamed('SJR', 'scimago_value')\\\n",
        "                        .withColumnRenamed('Issn', 'issn')\\\n",
        "                        .withColumnRenamed('Title', 'scimago_title')\\\n",
        "                        .transform(Operations.generate_key_from_journal_name, \n",
        "                                  journal_title_column = 'scimago_title', \n",
        "                                  output_column = 'journal_name_key')\n",
        "                        \n",
        "jcs_df = JCS_DF.select([\"Full Journal Title\", \"Journal Impact Factor\"])\\\n",
        "                .withColumnRenamed(\"Full Journal Title\", \"jcs_title\")\\\n",
        "                .withColumnRenamed(\"Journal Impact Factor\", \"jcs_value\")\\\n",
        "                .transform(Operations.generate_key_from_journal_name, \n",
        "                                  journal_title_column = 'jcs_title', \n",
        "                                  output_column = 'journal_name_key')\n",
        "\n",
        "########################################## JOIN dos arquivos CSV ##########################################\n",
        "journal_df = scimago_df.alias(\"scimago_df\")\\\n",
        "            .join(jcs_df, 'journal_name_key', 'outer')\\\n",
        "            .withColumn('title', F.coalesce(\"scimago_title\", \"jcs_title\"))\\\n",
        "            .withColumn(\"issn\", F.regexp_replace(\"issn\", \" \",\"\"))\\\n",
        "            .select(['title', 'journal_name_key', 'issn', 'scimago_value', 'jcs_value'])\\\n",
        "            .dropDuplicates([\"journal_name_key\"])\n",
        "            \n",
        "journal_df = journal_df.withColumn(\"issn\", F.when(journal_df[\"issn\"] == '-', None).otherwise(journal_df[\"issn\"]))\n",
        "\n",
        "splited_issn = F.split(journal_df[\"issn\"], \",\")\n",
        "\n",
        "journal_df = journal_df.withColumn('issn_1', splited_issn.getItem(0))\\\n",
        "                        .withColumn('issn_2', splited_issn.getItem(1))\\\n",
        "                        .withColumn('issn_3', splited_issn.getItem(3))\\\n",
        "                        .select(['title', 'journal_name_key', 'issn_1', 'issn_2', 'issn_3', 'scimago_value', 'jcs_value'])\n"
      ],
      "metadata": {
        "id": "gQasuiqj4Vu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################## JOIN entre Bibtexs e CSV's ##########################################\n",
        "journal_df.createOrReplaceTempView(\"journal_df\")\n",
        "bibtex_df.createOrReplaceTempView(\"bibtex_df\")\n",
        "\n",
        "\n",
        "df_final = SPARK.sql(\n",
        "\"\"\"\n",
        "      SELECT DISTINCT\n",
        "            bib.author\n",
        "            ,bib.title\n",
        "            ,bib.keywords\n",
        "            ,bib.abstract\n",
        "            ,bib.year\n",
        "            ,bib.type_publication\n",
        "            ,bib.doi\n",
        "            ,bib.issn\n",
        "            ,COALESCE(bib.journal, jor.title) journal\n",
        "            ,bib.source\n",
        "            ,jor.scimago_value  \n",
        "            ,jor.jcs_value\n",
        "      FROM \n",
        "            bibtex_df bib\n",
        "            LEFT JOIN journal_df jor\n",
        "                  ON (bib.issn = jor.issn_1\n",
        "                        OR bib.issn = jor.issn_2\n",
        "                        OR bib.issn = jor.issn_3\n",
        "                        OR bib.journal_name_key = jor.journal_name_key)\n",
        "                  AND bib.issn is not null \n",
        "                  AND bib.journal is not null;\"\"\")\n",
        "\n",
        "journal_df.unpersist()\n",
        "bibtex_df.unpersist()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QoUocKmEddzk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d6fe266-0c02-4610-f81f-33002b6e17c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[author: string, title: string, keywords: string, abstract: string, year: string, type_publication: string, doi: string, issn: string, isbn: string, journal: string, source: string, journal_name_key: string]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.write.mode(\"overwrite\") \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", f\"jdbc:sqlserver://dteam.ddns.net:1433;databaseName=faculdade-impacta;\") \\\n",
        "    .option(\"dbtable\", \"t_bibtex_extraidos_manualmente\") \\\n",
        "    .option(\"user\", \"guilherme.magalhaes\") \\\n",
        "    .option(\"password\", 'Senha123$') \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .save()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zZdTw5mBn61C",
        "outputId": "0fa66439-da0b-4b95-8a03-bb48c31d3588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-080185e56e12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"guilherme.magalhaes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"password\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Senha123$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"driver\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o277.save.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:229)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:233)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    }
  ]
}